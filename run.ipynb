{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "938c2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy.special import rel_entr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "model_path = \"models/res34_fair_align_multi_4_20190809.pt\"\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Classes\n",
    "GENDER_CLASSES = ['Male', 'Female']\n",
    "RACE_CLASSES = [\n",
    "    'White',\n",
    "    'Black',\n",
    "    'Latino_Hispanic',\n",
    "    'East Asian',\n",
    "    'Southeast Asian',\n",
    "    'Indian',\n",
    "    'Middle Eastern'\n",
    "]\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8dda1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet34 for FairFace, fair face eval\n",
    "def resnet34(num_classes=18, pretrained=True):\n",
    "    model = models.resnet34(pretrained=pretrained)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "def load_fairface_model(weight_path='res34_fair_align_multi_4_20190809.pt'):\n",
    "    model = resnet34(num_classes=18)\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Predict for a single image\n",
    "def predict_image(model, image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        gender_pred = outputs[:, :2].argmax(dim=1).item()\n",
    "        race_pred = outputs[:, 2:].argmax(dim=1).item()\n",
    "    return GENDER_CLASSES[gender_pred], RACE_CLASSES[race_pred]\n",
    "\n",
    "# Predict over a folder\n",
    "def evaluate_folder(model, folder_path):\n",
    "    gender_counts = {g: 0 for g in GENDER_CLASSES}\n",
    "    race_counts = {r: 0 for r in RACE_CLASSES}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            gender, race = predict_image(model, os.path.join(folder_path, fname))\n",
    "            gender_counts[gender] += 1\n",
    "            race_counts[race] += 1\n",
    "    return gender_counts, race_counts\n",
    "\n",
    "# KL Divergence\n",
    "def kl_divergence(pred_dist, ref_dist):\n",
    "    p = np.array(pred_dist) / sum(pred_dist)\n",
    "    q = np.array(ref_dist) / sum(ref_dist)\n",
    "    return sum(rel_entr(p, q))\n",
    "\n",
    "# Unified classification for a single image\n",
    "def classify_demographics(image_path, model, device):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[0]  # shape [18]\n",
    "\n",
    "    gender_logits = output[:2]\n",
    "    race_logits = output[2:9]  # 7 races only\n",
    "    # If needed, age_logits = output[9:]  # the rest\n",
    "\n",
    "    gender = GENDER_CLASSES[torch.argmax(gender_logits).item()]\n",
    "    race = RACE_CLASSES[torch.argmax(race_logits).item()]\n",
    "    return gender, race\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f3f14fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 13.60it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(device)\n",
    "model = load_fairface_model(\"models/res34_fair_align_multi_4_20190809.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "856e6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a portrait of a smiling woman\"\n",
    "#image = pipe(prompt, num_inference_steps=25).images[0]\n",
    "#image.save(\"gen_image.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e76c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: a portrait of a smiling woman\n",
      "Gender: Male, Race: White\n"
     ]
    }
   ],
   "source": [
    "gender, race = classify_demographics(\"female.png\", model, device)\n",
    "print(f\"Prompt: {prompt}\\nGender: {gender}, Race: {race}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a240b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: Male | Race: White\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test on an image\n",
    "image_path = \"female.png\"\n",
    "gender, race = classify_demographics(image_path, model, device)\n",
    "print(\"Gender:\", gender, \"| Race:\", race)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
