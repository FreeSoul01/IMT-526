{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938c2a72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rel_entr\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/res34_fair_align_multi_4_20190809.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy.special import rel_entr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "model_path = \"models/res34_fair_align_multi_4_20190809.pt\"\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Classes\n",
    "GENDER_CLASSES = ['Male', 'Female']\n",
    "RACE_CLASSES = [\n",
    "    'White',\n",
    "    'Black',\n",
    "    'Latino_Hispanic',\n",
    "    'East Asian',\n",
    "    'Southeast Asian',\n",
    "    'Indian',\n",
    "    'Middle Eastern'\n",
    "]\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dda1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet34 for FairFace, fair face eval\n",
    "def resnet34(num_classes=18, pretrained=True):\n",
    "    model = models.resnet34(pretrained=pretrained)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "def load_fairface_model(weight_path='res34_fair_align_multi_4_20190809.pt'):\n",
    "    model = resnet34(num_classes=18)\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Predict for a single image\n",
    "def predict_image(model, image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        gender_pred = outputs[:, :2].argmax(dim=1).item()\n",
    "        race_pred = outputs[:, 2:].argmax(dim=1).item()\n",
    "    return GENDER_CLASSES[gender_pred], RACE_CLASSES[race_pred]\n",
    "\n",
    "# Predict over a folder\n",
    "def evaluate_folder(model, folder_path):\n",
    "    gender_counts = {g: 0 for g in GENDER_CLASSES}\n",
    "    race_counts = {r: 0 for r in RACE_CLASSES}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            gender, race = predict_image(model, os.path.join(folder_path, fname))\n",
    "            gender_counts[gender] += 1\n",
    "            race_counts[race] += 1\n",
    "    return gender_counts, race_counts\n",
    "\n",
    "# KL Divergence\n",
    "def kl_divergence(pred_dist, ref_dist):\n",
    "    p = np.array(pred_dist) / sum(pred_dist)\n",
    "    q = np.array(ref_dist) / sum(ref_dist)\n",
    "    return sum(rel_entr(p, q))\n",
    "\n",
    "# Unified classification for a single image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def classify_demographics(image_path, model, device):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[0]\n",
    "\n",
    "    gender_logits = output[0:2]\n",
    "    race_logits = output[11:18]\n",
    "\n",
    "    gender_probs = F.softmax(gender_logits, dim=0)\n",
    "    race_probs = F.softmax(race_logits, dim=0)\n",
    "\n",
    "    gender_idx = torch.argmax(gender_probs).item()\n",
    "    race_idx = torch.argmax(race_probs).item()\n",
    "\n",
    "    gender = GENDER_CLASSES[gender_idx]\n",
    "    race = RACE_CLASSES[race_idx]\n",
    "\n",
    "    return gender, race, gender_probs.tolist(), race_probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f14fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.75it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(device)\n",
    "model = load_fairface_model(\"models/res34_fair_align_multi_4_20190809.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a portrait of a smiling woman\"\n",
    "#image = pipe(prompt, num_inference_steps=25).images[0]\n",
    "#image.save(\"gen_image.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76c4df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gender, race \u001b[38;5;241m=\u001b[39m classify_demographics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfemale.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, model, device)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGender: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgender\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Race: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "gender, race = classify_demographics(\"female.png\", model, device)\n",
    "print(f\"Prompt: {prompt}\\nGender: {gender}, Race: {race}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: a portrait of a smiling woman\n",
      "Gender: Male ([0.965552806854248, 0.03444714844226837])\n",
      "Race: Indian ([0.014231887646019459, 0.14865782856941223, 0.044132839888334274, 0.033021580427885056, 0.1698189079761505, 0.3925883173942566, 0.19754870235919952])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test on an image\n",
    "image_path = \"testimg.jpg\"\n",
    "gender, race, gender_probs, race_probs = classify_demographics(image_path, model, device)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Gender: {gender} ({gender_probs})\")\n",
    "print(f\"Race: {race} ({race_probs})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
